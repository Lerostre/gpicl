{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использованные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a3rmOh1oCLJH",
    "outputId": "99a73ce9-0770-4a92-88ef-f6c019107f89",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pl_base import BestValidCallback\n",
    "from datagen import SubLoader, TaskAugmentor\n",
    "from models import GPT\n",
    "from utils import gpt_params, g_transform\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Учимся учиться"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Одна из самых интересных вещей в этой статье - то, что трансформер, если ему дать достаточно много задач, начинает не просто запоминать задачи, а вычленять общие закономерности, чтобы решать такие задачи, которых он ещё не видел. Мы будем смотреть это через такой же метатест, что и в `3. Metatrain.ipynb`. Вкратце: даём модели последовательность в 99 примеров, смотрим на качество на 100-м. Разнциа лишь в том, что интересует нас теперь лишь трансформер, а также смотреть будем не только на 100-й пример, но и на аккураси в целом по мере обработки последовательности\n",
    "\n",
    "На картинке ниже показан график улучшения аккураси в зависимости от числа тасок. Суть здесь в том, что аккураси у каждого отдельного элемента последовательности может быть сколь угодно высокий, например `[0.9., 1., 0.9, 1.]`, но значимое улучшение составит лишь 0.1, модель не научилась обрабатывать таску в совокупности, она просто запомнила какое-то оптимальное решение задачи. Если же улучшение наоборот большое, то модель хорошо обобщает и вне зависимости от последовательности выдаёт правильный ответ, потому что научилась учиться. Чем больше тасок, тем больше эффект, фазовый переход происходит примерно на числе в $2^{13}$\n",
    "\n",
    "<img src=\"https://cdn.discordapp.com/attachments/674191702906503199/1194420922837372999/image.png?ex=65b04a2b&is=659dd52b&hm=04fc28218f048fa96b685c2df1380f998078f7561b1a8ba18b1b20aeae248022&\" width=\"500px\">\n",
    "\n",
    "Помимо этого посмотрим на сам график улучшения аккураси. В случае трансформера это можно сделать через маску - так, чтобы модель видела лишь часть данных при предсказании, а аккураси будет означать качество на 100-м лейбле. Если качество растёт, хоть как-то, то модель научилась извлекать информацию из последовательности - то есть научилась учиться. В нашем сетапе будем делать это для трансформера на $2^{16}$ тасках, как и в статье\n",
    "\n",
    "<img src=\"https://cdn.discordapp.com/attachments/674191702906503199/1194422652597710919/image.png?ex=65b04bc7&is=659dd6c7&hm=596a6d1b05da565f078bfb7dcf3961097b65cc6ad88cc096c66154d4e457d814&\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конфиг, трансформы и датасеты здесь такие же, как в предыдущих экспериментах, как самые удачные. Нам понадобится 3 валидации: MNIST с теми же тасками, MNIST с новыми тасками и FashionMNIST с новыми тасками. Учиться будем либо на MNIST, либо на FashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_test = SubLoader(MNIST('./datasets', train=True, transform=g_transform))\n",
    "fashion_mnist_test = SubLoader(FashionMNIST('./datasets', train=True, transform=g_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчёта аккураси понадобится кастомная функция. Она генерирует маски до $i$-го токена, делает предсказание модели, имея в виду лишь $i$ токенов, подсчитывает качество для каждого $i$ и его общий прирост - разница в аккураси между обучением на 99 и обучением на 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seq_accuracy(loader):\n",
    "    res = torch.zeros(99)\n",
    "    for i in range(99):\n",
    "        attention_mask = torch.zeros(len(loader), 100)\n",
    "        attention_mask[:, :i] = 1\n",
    "        pred = F.softmax(\n",
    "            model(loader.dataset.data, attention_mask=attention_mask), 1\n",
    "        ).argmax(1)\n",
    "        res[i] += (pred[:, -1] == loader.dataset.targets[:, -1]).float().mean()\n",
    "    return res, res[-1] - res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логику вытаскивания даталоадеров лучше сразу засунуть в функцию. Её лучше вообще не менять. Число тасок в тесте это более изящный способ фиксации сида - оцениваем по каждой, затем усредняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(\n",
    "    n_tasks, draw_sequence, batch_size, train_dataset, seed\n",
    "):\n",
    "    train_samples = 1\n",
    "    train_augmentor = TaskAugmentor(n_tasks, draw_sequence=True, random_state=seed)\n",
    "    seen_augmentor = TaskAugmentor(256, draw_sequence=True, random_state=seed)\n",
    "    unseen_augmentor = TaskAugmentor(256, draw_sequence=True, random_state=seed+1)\n",
    "    train_loader = DataLoader(\n",
    "        train_augmentor.transform(train_dataset, train_samples),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        seen_augmentor.transform(mnist_test, 1),\n",
    "        batch_size=1, shuffle=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        unseen_augmentor.transform(mnist_test, 1),\n",
    "        batch_size=1, shuffle=False\n",
    "    )\n",
    "    test_fashion_loader = DataLoader(\n",
    "        unseen_augmentor.transform(fashion_mnist_test, 1),\n",
    "        batch_size=1, shuffle=False\n",
    "    )\n",
    "    return train_loader, valid_loader, test_loader, test_fashion_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже запускает обучение и логгирует результаты на wandb, [там](https://wandb.ai/lerostre/gpicl?workspace=user-lerostre) можно на них полюбоваться. Ключевая проблема - модели реально долго учить, они выходят на плато, поэтому EarlyStopping. результаты как обычно в датафрейм. Я запускал только для одного сида, потому что нету времени, а результаты и так видно\n",
    "\n",
    "Основные параметры - трейн, у нас их 2, число тасок для перебора, таски, на которых замеряем улучшение аккураси, батч сайз для даталоадера, куда сохранять 100-е аккураси, куда сохранять аккураси от 1 до 99, сид, и конфиг для трансформера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_for_sequence_improvement(\n",
    "    train_dataset: SubLoader,\n",
    "    n_tasks_list: np.ndarray = 2**np.arange(0, 19),\n",
    "    n_tasks_for_full_check: int = [2**16],\n",
    "    batch_size: int = 128,\n",
    "    output_csv: str = \"last_acc.csv\",\n",
    "    seq_res_output: str = \"mnist_seq_acc\",\n",
    "    random_state: int = 69,\n",
    "    gpt_params = gpt_params,\n",
    "    log=True\n",
    "):\n",
    "    logs = []\n",
    "    for n_tasks in n_tasks_list:\n",
    "\n",
    "        wandb_logger = None\n",
    "        if log:\n",
    "            # change if necessary\n",
    "            wandb.init(project=\"gpicl\", name=f\"gpt_2^{int(np.log2(n_tasks))}\", tags=[\"gpt\"])\n",
    "            wandb_logger = WandbLogger(log_model=False)\n",
    "        # get loaders=[train, \"seen_mnist\", \"unseen_mnist\", \"unseen_fashion_mnist\"]\n",
    "        loaders = augment_data(\n",
    "            n_tasks, draw_sequence=True, seed=random_state,\n",
    "            batch_size=batch_size, train_dataset=train_dataset\n",
    "        )\n",
    "        model_name, model = (\"transformer\", GPT(**gpt_params))\n",
    "\n",
    "        # training loop, free to adjust\n",
    "        trainer = pl.Trainer(\n",
    "            precision=\"16\",\n",
    "            accelerator=\"gpu\",\n",
    "            logger=wandb_logger,\n",
    "            enable_progress_bar=False,\n",
    "            max_steps=100000,\n",
    "            enable_model_summary=False,\n",
    "            callbacks=[EarlyStopping(\n",
    "                monitor=\"train_accuracy\",\n",
    "                min_delta=0.025,\n",
    "                patience=100,\n",
    "                mode=\"max\"\n",
    "            )]\n",
    "        )\n",
    "        trainer.fit(model, loaders[0], loaders[1:])\n",
    "\n",
    "        # dataframe store\n",
    "        for idx, name in enumerate([\n",
    "            \"seen_mnist\", \"unseen_mnist\", \"unseen_fashion_mnist\"\n",
    "        ]):\n",
    "            seq_acc, last_acc = seq_accuracy(loaders[idx+1])\n",
    "            if n_tasks in n_tasks_for_full_check:\n",
    "                prefix = f\"experiments/{seq_res_output}_{n_tasks}\"\n",
    "                np.save(prefix+\"_\"+name, seq_acc)\n",
    "            entry = {\n",
    "                \"n_tasks\": n_tasks, \"dataset\": name,\n",
    "                \"last_acc\": last_acc.item(),\n",
    "            }\n",
    "            logs.append(entry)\n",
    "            pd.DataFrame(logs).to_csv(f\"experiments/{output_csv}\", index=0)\n",
    "        clear_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала запустить для MNIST, потом для FashionMNIST, вот и вся задача. Самое сложное - дождаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = SubLoader(MNIST('./datasets', train=True, transform=g_transform)) \n",
    "train_for_sequence_improvement(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = SubLoader(FashionMnist('./datasets', train=True, transform=g_transform)) \n",
    "train_for_sequence_improvement(\n",
    "    train_dataset=mnist_train,\n",
    "    n_tasks_list=[2**16]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Интерпретация\n",
    "\n",
    "Теперь будем строить графики по порядку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "set_mapper = {\n",
    "    \"seen_mnist\": \"Seen MNIST\",\n",
    "    \"unseen_mnist\": \"Unseen MNIST\",\n",
    "    \"unseen_fashion_mnist\": \"Unseen FashionMNIST\"\n",
    "}\n",
    "df = pd.read_csv(\"experiments/last_acc.csv\")\n",
    "df[\"dataset\"] = df.dataset.map(set_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    df, \"n_tasks\", \"last_acc\", color=\"dataset\",\n",
    "    color_discrete_sequence=[\"crimson\", \"midnightblue\", \"goldenrod\"]\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    type='category', tickvals=df.n_tasks.values,\n",
    "    ticktext = [f\"$2^{{{int(np.log2(i))}}}$\" for i in df.n_tasks.values]\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=0.02,\n",
    "        y=0.94,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=2,\n",
    "        title=\"Dataset\"\n",
    "    ),\n",
    "    width=700, height=500, title=\"Ability to learn per number of tasks\",\n",
    "    yaxis=dict(title=\"Accuracy improvement in sequence\"),\n",
    "    xaxis=dict(title=\"Number of tasks\"),\n",
    ")\n",
    "fig.add_vrect(\n",
    "    x0=0, x1=5, \n",
    "    annotation_text=\"Instance memorization\", annotation_position=\"top right\",\n",
    "    fillcolor=\"gold\", opacity=0.05, line_width=0,\n",
    "    label=dict(font=dict(size=12)),\n",
    ")\n",
    "fig.add_vrect(\n",
    "    x0=5, x1=7, \n",
    "    annotation_text=\"Task memorization\", annotation_position=\"top right\",\n",
    "    fillcolor=\"blue\", opacity=0.05, line_width=0,\n",
    "    label=dict(font=dict(size=12)),\n",
    ")\n",
    "fig.add_vrect(\n",
    "    x0=7, x1=9, \n",
    "    annotation_text=\"Learning to learn\", annotation_position=\"top left\",\n",
    "    fillcolor=\"red\", opacity=0.05, line_width=0,\n",
    "    label=dict(font=dict(size=12)),\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.discordapp.com/attachments/674191702906503199/1194466609058811975/newplot_1.png?ex=65b074b7&is=659dffb7&hm=2006b8cee485c0a5ca054d4814c1356689931dc6605169f3ed2d468a9daf1f28&\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом график получается очень похожим, только качество слегка ниже, чем в статье. Но самое главное мы видим - модель действительно учится обощать при растущем количестве тасок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([\n",
    "    pd.DataFrame(np.load(f\"experiments/seq_acc_{dataset}.npy\"))\n",
    "    for dataset in list(set_mapper)[1:]\n",
    "])\n",
    "res[\"dataset\"] = [y for x in list(set_mapper)[1:] for y in [x]*99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    res, y=0, color=\"dataset\",\n",
    "    color_discrete_sequence=[\"midnightblue\", \"goldenrod\"]\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=0.54,\n",
    "        y=0.20,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"Black\",\n",
    "        borderwidth=2,\n",
    "        title=\"Dataset\"\n",
    "    ),\n",
    "    width=700, height=500, title=\"Ability to learn per number of tasks\",\n",
    "    yaxis=dict(title=\"Accuracy improvement in sequence\"),\n",
    "    xaxis=dict(title=\"Number of tasks\"),\n",
    ")\n",
    "fig.update_xaxes(range=[-1, 101])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.discordapp.com/attachments/674191702906503199/1194466836012605481/newplot_2.png?ex=65b074ed&is=659dffed&hm=105d8d23f1bf527a4daa946d9c0155148c7932174caa594ab38a67546d254c2e&\"> "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
