## Отчет <a name="report"></a>

Это отчёт по воспроизведению результатов статьи [GPICL:
General Purpise In Context Learning](https://arxiv.org/pdf/2212.04458.pdf). В ней рассказывается об удивительном свойстве трансформеров обучаться учиться (learning-to-learn), если предоставить им достаточное количество данных. При этом это совсем не обязательны какие-то уникальные данные, достичь такого эффекта можно самыми обычными преобразованиями - проекцией и перестановкой. Также мы посмотрим, как хороши трансформеры в метаобучении, действительно они обобщают или нет, и при каких параметрах это происходит.

### [1. Аугментации данных](https://nbviewer.org/github/Lerostre/gpicl/blob/main/1.%20Task%20generation.ipynb)

Главная мысль статьи - нам не нужно ни большое количество данных, ни какие-то хитрые их аугментации, чтобы обучить модель, которая будет обладать большой обобщающей способностью. Ключевая мысль - взять имеющийся датасет и, путём простых преобразований - линейной проекции инпутов, перестанвокой таргетов - нагенерировать выборку сколь угодно большую. Ниже то, как это должно выглядеть по задумке

<img src="src/task_ref.png" width="700px">

Эта часть реализована в классе `TaskAugmentor`. Впрочем, авторы статьи отмечают, что это может быть достаточно трудоёмкий процесс, настолько, что требуется 16 GPU. Это действительно так, генерация, сэмплирование, хранение тратят много времени.

### [2. Влияние параметров](https://nbviewer.org/github/Lerostre/gpicl/blob/main/2.%20Hparam%20sweep.ipynb)

Обучение нейросетей заключается в подаче на вход как можно большего количества самых разных объектов и задач, чтобы в итоге получить модель, которая может решать большое число задач. Авторы статьи как раз исследуют, какое число аугментаций и какая сложность модели оптимальны, чтобы достичь нужного эффекта. 

Смотреть это мы будем на примере перцептрона и трансформера. В статье берётся, по всей видимости, encoder-decoder, здесь же decoder-only. Входные данные в статье не уточняются, хотя говорится, что это $[x_1, ... x_n, y1, ... y_m]$ - конкатенация инпута и таргета, но как можно спроецировать из $mathcal{R}$ в дискретное число эмбеддингов? Возможно, что из-за этого результаты не такие внушительные, но они всё же есть

<img src="src/hparam_heatmap.png" width="700px">

На графике мы видим интересную особенность. Перцептрон, сколько бы параметров у него ни было, может только запоминать задачи, но не обобщать. Трафнсформер же, если дать достаточно данных, начинает работать, как мета-модель - он учится прямо из контекста, и это по-настоящему поразительно, подробнее это исследуем ниже.

### [3. Сравнение моделей](https://nbviewer.org/github/Lerostre/gpicl/blob/main/3.%20Model%20comparison.ipynb) 

Трансформер это не единственная архитектура, которую можно изучать в этом отношении. Как минимум, есть ещё и LSTM (у нас обычный, не получилось сделать Outerproduct-LSTM из статьи, слишком времязатратно), VSML (его тоже нет, он написан на `jax`, некогда с ним разбираться, хотя код мы нашли), и MAML (он готов, но не хватило времени).

Мы будем сравнивать их качество следующим образом:

1. MLP и MAML это линейные модели, мы будем подавать им 99 примеров, делать градиентный спуск, смотреть на качество на 100.
2. LSTM и Transformer - модели seq2seq, они будут принимать 99 объектов, а качество будем сравнивать на последнем векторе контекста по 100 примеру.

Обучать всё будем на аугментированном MNIST с числом тасок $2^{16}$. В статье результаты куда более внушительные, но самое главное у нас тоже видно.

Показатели в статье по части моделей были следующие:
$$
\begin{array}{lllc}
\hline \text{**Method / Dataset** } & \text{**MNIST**} & \text{**Fashion MNIST** }  & \text{**KMNIST** } & \text{**CIFAR10** } & \text{**SVHN** }\\
\hline \text {SGD} & \text{0.7031} & \text{0.5078}  & \text{0.3789} & \text{0.1484} & \text{0.1016} \\
\text {LSTM (outer-product)} & \text{0.2539} & \text{0.2812}  & \text{0.1810} & \text{0.1211} & \text{0.1107} \\
\text {GPICL Transformer} & \text{0.7370} & \text{0.6224}  & \text{0.5339} & \text{0.1940} & \text{0.1458} \\
\hline
\end{array}
$$

А здесь то, что получилось у нас:

$$
\begin{array}{lllc}
\hline \text{**Method / Dataset** } & \text{**MNIST**} & \text{**Fashion MNIST** }  & \text{**KMNIST** } & \text{**CIFAR10** } & \text{**SVHN** }\\
\hline \text {MLP} & \text{0.370968} & \text{0.225806}  & \text{0.096774} & \text{0.081967} & \text{0.049180} \\
\text {LSTM} & \text{0.109375} & \text{0.095052}  & \text{0.098958} & \text{0.104167} & \text{0.102865} \\
\text {GPT (GPICL)} & \text{0.523438} & \text{0.458333}  & \text{0.350260} & \text{0.114583} & \text{0.088542} \\
\hline
\end{array}
$$

К сожалению, сложно сказать, что у нас конкретно пошло не так, но все модели показывают качество заметно хуже. Трансформер, возможно, из-за decoder-only архитектуры, но с перцептроном всё же непонятно. Возможно, связано с тем, что все инпуты были 28x28x1, а не 32x32x3, как в статье. Это сделано просто из соображений экономии памяти.

Тем не менее, трансформер вновь показывает, что он выучил какие-то такие вещи, которые позволяют ему на совершенно любом датасете показывать неплохое качество. И скорее всего, этот его потенциал можно многократно развить. Единственная проблема - он выходит на плато, его действительно сложно обучать, доказательство этому есть на [wandb](https://wandb.ai/lerostre/gpicl?workspace=user-lerostre)

### [4. Обучение в контексте](https://nbviewer.org/github/Lerostre/gpicl/blob/main/4.%20Sequence%20metatest.ipynb)

Чтобы убедиться в том, что GPICL действительно черпает информацию из контекста, посмотрим на то, как он от этого контекста зависит. Сперва взглянем на график улучшения accuracy в пределах последовательности. Если он равно 0, значит, модели не важно, 0 объектов ей дано на вход, или все 100, она просто запомнила оптимальный ответ либо оптимальное решение задачи. Мы же хотим, чтобы она запомнила, как этой задаче обучиться. Про это график ниже:

<img src="src/seq_improv.png" width="700px">

Видно, что начиная с определённого момента, где-то в $2^{14}$ задач, происходит фазовый фереход - модель действительно становится мета-моделью.

Ещё одна вещь, на которую можно посмотреть - как же всё-таки это улучшение происходит. Можно замерить accuracy в зависимости от поданного числа инпутов. Если оно растёт - значит, модель эти знания действительно извлекает. Если нет - модель запоминает

<img src="src/per_seq_acc.png" width="700px">

И вновь мы видим, что трансформер это свойство демонстрирует, а значит, потенциально, может использоваться для решения любой задачи, по всей видимости. Это ли не чудо?

### Содержание репозитория

1. [**1. Task generation.ipynb**](https://nbviewer.org/github/Lerostre/gpicl/blob/main/1.%20Task%20generation.ipynb) - Здесь подробное описание того, как генерировать новые таски для задачи обучения GPICL, вместе с примерами
2. [**2. Hparam sweep.ipynb**](https://nbviewer.org/github/Lerostre/gpicl/blob/main/2.%20Hparam%20sweep.ipynb) - Тут мы посмотрим на то, почему трансформер может обобщать, а перцептрон только лишь запоминать, проверим качество на старых и новых для модели тасках в завимиости от параметров
3. [**3. Model comparison.ipynb**](https://nbviewer.org/github/Lerostre/gpicl/blob/main/3.%20Model%20comparison.ipynb) - Тут попробуем сравнить другие модели в роли метамоделей для задачи предсказания 100 объекта, зная 99
4. [**4. Sequence metatest**](https://nbviewer.org/github/Lerostre/gpicl/blob/main/4.%20Sequence%20metatest.ipynb) - Тут посмотрим, как же именно трансформер обобщает и учится извлекать информацию из последовательности
5. [**datagen.py**](https://github.com/Lerostre/gpicl/blob/main/datagen.py) - Здесь всё, что нужно для аугментации данных - проекции, перестановки, интерфейс для обработки датасетов с картинками
6. [**models.py**](https://github.com/Lerostre/gpicl/blob/main/models.py) - Здесь использованные в работе модели
7. [**pl_base.py**](https://github.com/Lerostre/gpicl/blob/main/pl_base.py) - Тут общий для всех моделей интерфейс для обучения с `pytorch-lightning`
7. [**utils.py**](https://github.com/Lerostre/gpicl/blob/main/utils.py) - Здесь в основном то, чему не нашлось места в других модулях. Например, дефолтеые конфиги для моделей
8. [**readme.md**](#report) - Это собственно отчёт о проделанной работе
9. **/experiments/** - В этой папке все эксперименты. В них не так просто разобраться по названиям, лучше смотреть прежде всего в ноутбуки
9. **/src/** - Здесь лежат картинки для отчёта
